# ============================================
# LLM CONFIGURATION
# ============================================
# Direct model specification (provider/model format)
# Examples:
#   - openai/gpt-4o-mini
#   - anthropic/claude-3-opus-20240229  
#   - ollama/gemma3:27b
#   - gemini/gemini-1.5-pro
#   - cohere/command-r-plus
#   - replicate/meta/llama-2-70b-chat
LLM_MODEL=openai/gpt-4o-mini

# LLM Generation Settings
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
LLM_RETRIES=3
LLM_CACHE=true

# For local models (Ollama, vLLM, etc), pass api_base directly:
# Example: setup_llm("ollama/llama3.2", api_base="http://localhost:11434")

# ============================================
# API KEYS (required based on provider)
# ============================================
# OpenAI (required for openai/* models)
# OPENAI_API_KEY=your-openai-api-key-here

# Anthropic Claude (required for anthropic/* models)
# ANTHROPIC_API_KEY=your-claude-api-key-here

# Google Gemini (required for gemini/* models)
# GOOGLE_API_KEY=your-gemini-api-key-here

# Cohere (required for cohere/* models)
# COHERE_API_KEY=your-cohere-api-key-here

# Replicate (required for replicate/* models)
# REPLICATE_API_TOKEN=your-replicate-token-here

# ============================================
# DEBUG & MONITORING
# ============================================
# Debug Mode - Shows DSPy prompts and LLM responses
DSPY_DEBUG=false

# Agent Loop Configuration
AGENT_MAX_ITERATIONS=5
AGENT_TIMEOUT_SECONDS=60.0